# -*- coding: utf-8 -*-
"""Creating_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kig5YCUcog1pmO2ofi3efyK3tG3EL8j-
"""


import pandas as pd
import numpy as np
import joblib
# Set seed for reproducibility
np.random.seed(42)

# Define constants
n_stores = 5
n_products = 10
weeks = list(range(1, 53))
seasons = ['winter', 'spring', 'summer', 'autumn']

# Simulate dataset with predictable patterns
data = []

for store_id in range(1, n_stores + 1):
    for product_id in range(1, n_products + 1):
        prev_sales = np.random.randint(40, 70)
        for week in weeks:
            is_holiday = int(week in [1, 5, 10])
            is_weekend = int(week % 6 == 0 or week % 7 == 0)
            season = seasons[(week - 1) // 3 % 4]
            discount_percent = np.random.choice([0, 10, 20, 30])
            price_per_unit = round(10 + np.random.randn() * 2, 2)

            # Predictable influence pattern
            effect = (
                (discount_percent * 0.5) +
                (is_holiday * 8) +
                (is_weekend * 4) -
                (price_per_unit * 0.3) +
                np.random.normal(0, 2)
            )
            next_sales = max(0, prev_sales + effect)

            data.append({
                'week_of_year': week,
                'is_holiday': is_holiday,
                'is_weekend': is_weekend,
                'season': season,
                'discount_percent': discount_percent,
                'price_per_unit': price_per_unit,
                'storeid': store_id,
                'productid': product_id,
                'prev_week_sales': prev_sales,
                'next_week_sales': next_sales
            })

            prev_sales = next_sales

# Create DataFrame
predictable_df = pd.DataFrame(data)
predictable_df['season'] = predictable_df['season'].astype('category').cat.codes
predictable_df.head(20)

df = predictable_df

from sklearn.preprocessing import MinMaxScaler

# Feature lists
all_features = [
    'week_of_year', 'is_holiday', 'is_weekend', 'season',
    'discount_percent', 'price_per_unit', 'storeid', 'productid', 'prev_week_sales'
]
target = 'next_week_sales'

# Features to normalize
features_to_normalize = ['is_holiday', 'is_weekend', 'season',
                         'discount_percent', 'price_per_unit', 'prev_week_sales']

# Create a copy to avoid modifying original df
df_scaled = df.copy()

# Normalize only selected features
scaler = MinMaxScaler()
df_scaled[features_to_normalize] = scaler.fit_transform(df_scaled[features_to_normalize])

df_scaled.head()
df = df_scaled
print(df)

features = [
    'week_of_year', 'is_holiday', 'is_weekend', 'season',
    'discount_percent', 'price_per_unit', 'storeid', 'productid', 'prev_week_sales'
]
target = 'next_week_sales'

train = df[df['week_of_year'] <= 40]
test  = df[df['week_of_year'] > 40]

df['week_of_year'].max()

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lr = LinearRegression()
lr.fit(train[features],train[target] )
preds = lr.predict(test[features])

print("Linear Regression MSE:", mean_squared_error(test[target], preds))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(train[features],train[target] )
preds = rf.predict(test[features])

print("Random Forest MSE:", mean_squared_error(test[target], preds))

from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gbr.fit(train[features],train[target] )
preds = gbr.predict(test[features])

print("Gradient Boosting MSE:", mean_squared_error(test[target], preds))

import xgboost as xgb

xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(train[features],train[target] )
preds = xgb_model.predict(test[features])

print("XGBoost MSE:", mean_squared_error(test[target], preds))

# Step 1: make sure you generate predictions for all test rows
 # preds will be len(test) size

# Step 2: add predictions as a new column to the test set
test = test.copy()
test['prediction'] = preds

# Step 3: now you can filter by store/product safely
store_id = 5
product_id = 4

sample = test[(test['storeid'] == store_id) & (test['productid'] == product_id)].copy()
sample = sample.sort_values(by='week_of_year')

# Step 4: plot
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(sample['week_of_year'], sample['next_week_sales'], label='Actual', marker='o')
plt.plot(sample['week_of_year'], sample['prediction'], label='Predicted', marker='x')
plt.title(f'Sales Forecast (Store {store_id}, Product {product_id})')
plt.xlabel('Week')
plt.ylabel('Sales')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import shap

train_shap = train.drop('next_week_sales', axis = 1)

# for i in range(train_shap.shape[1]):
#   print(train_shap.columns[i], "=", lr.coef_[i].round(5))

# train.to_csv('old.csv', index=False)
# test.to_csv('new.csv', index=False)
# joblib.dump(lr, 'linear_regression_model.joblib')

explainer = shap.Explainer(lr, train_shap)
shap_values = explainer(train_shap)
print(shap_values)